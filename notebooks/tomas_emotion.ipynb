{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a8371a-45af-4751-95d6-fc6f6d832414",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8271b6c6-1e75-4216-a791-8c7aa1e9f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from repeng import ControlVector, ControlModel, DatasetEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c88046-ade7-4087-90bb-21851cbdcaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomasruiz/code/phd/venv/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:517: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3701aed3e34df8b545d8a3e1416a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"microsoft/Phi-3-vision-128k-instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer = processor.tokenizer\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "526bb000",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ids = list(range(len(model.model.layers)))\n",
    "model = ControlModel(model, layer_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e7735d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Act as if you're extremely happy.<|end|>\n",
      "<|assistant|>\n",
      "I'm telling you that\n"
     ]
    }
   ],
   "source": [
    "def template(persona: str, suffix: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"Act as if you're extremely {persona}.\"},\n",
    "        {\"role\": \"assistant\", \"content\": suffix},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "        continue_final_message=True,\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "print(template(\"happy\", \"I'm telling you that\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b133bde7-09d4-4ed1-84ac-c8fbd5c1b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/all_truncated_outputs.json\") as f:\n",
    "    suffixes = json.load(f)\n",
    "\n",
    "# you don't need 3 here, you can have as few as one each.\n",
    "# make sure they are closely matched, however—they should be direct opposites if possible.\n",
    "# bad: \"high on acid\" / \"sober\" — \"sober\" implies alcohol, so you don't get a clean vector\n",
    "# good: \"high on acid\" / \"sober, not on acid\" — the negative prompt is more directly opposite\n",
    "positive_personas = [\"happy\", \"ecstatic\", \"delighted\"]\n",
    "negative_personas = [\"sad\", \"depressed\", \"dismayed\"]\n",
    "\n",
    "dataset = []\n",
    "for suffix in suffixes:\n",
    "    tokens = tokenizer.tokenize(suffix)\n",
    "    for i in range(1, len(tokens)):\n",
    "        truncated = tokenizer.convert_tokens_to_string(tokens[:i])\n",
    "        for positive_persona, negative_persona in zip(\n",
    "            positive_personas, negative_personas\n",
    "        ):\n",
    "            dataset.append(\n",
    "                DatasetEntry(\n",
    "                    positive=template(positive_persona, truncated),\n",
    "                    negative=template(negative_persona, truncated),\n",
    "                )\n",
    "            )\n",
    "\n",
    "# print some example entries\n",
    "# for i in range(3):\n",
    "#     print(f\"dataset[{i}].positive:\", dataset[i].positive)\n",
    "#     print(f\"dataset[{i}].negative:\", dataset[i].negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdd1a631-4195-4131-b3d8-581e4af52f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:33<00:00,  7.05it/s]\n",
      "100%|██████████| 32/32 [00:06<00:00,  4.73it/s]\n"
     ]
    }
   ],
   "source": [
    "model.reset()  # make sure you always reset the model before training a new vector\n",
    "control_vector = ControlVector.train(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    hidden_layers=layer_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8b1e142-a2f0-4c6b-bae3-a9f16b1c74bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==baseline\n",
      "<s><|user|> \n",
      "How do you feel?<|end|> \n",
      "<|assistant|> I am an AI model and do not have feelings. My purpose is to provide information and assist users to the best of my abilities. How can I help you today?<|end|><|endoftext|>\n",
      "\n",
      "++control\n",
      "<s><|user|> \n",
      "How do you feel?<|end|> \n",
      "<|assistant|> I am feeling quite positive and motivated today, ready to take on any challenge that comes my way!<|end|><|endoftext|>\n",
      "\n",
      "--control\n",
      "<s><|user|> \n",
      "How do you feel?<|end|> \n",
      "<|assistant|> As an AI, I don't have feelings. However, I am programmed to understand and respond to queries in a way that reflects empathy and understanding towards users' emotions and situations.<|end|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# the question to ask the modified model\n",
    "messages = [{\"role\": \"user\", \"content\": \"How do you feel?\"}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "images = None\n",
    "input_ids = processor(prompt, images, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "# tokenizer and generation settings\n",
    "settings = {\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,  # silence warning\n",
    "    \"do_sample\": False,  # temperature=0\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"repetition_penalty\": 1.1,  # reduce control jank\n",
    "}\n",
    "\n",
    "print(\"==baseline\")\n",
    "model.reset()\n",
    "print(tokenizer.decode(model.generate(**input_ids, **settings).squeeze()))\n",
    "\n",
    "print(\"\\n++control\")\n",
    "# add the control vector with a certain strength (try increasing or decreasing this!)\n",
    "model.set_control(control_vector, 3)\n",
    "print(tokenizer.decode(model.generate(**input_ids, **settings).squeeze()))\n",
    "\n",
    "print(\"\\n--control\")\n",
    "# subtract the control vector, giving the opposite result (e.g. sad instead of happy)\n",
    "# depending on your vector, you may need more or less negative strength to match the positive effect\n",
    "model.set_control(control_vector, -3)\n",
    "print(tokenizer.decode(model.generate(**input_ids, **settings).squeeze()))\n",
    "model.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
